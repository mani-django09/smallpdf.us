# robots.txt for SmallPDF.us
# https://smallpdf.us/robots.txt

# Allow all search engines to crawl the site
User-agent: *
Allow: /

# Block admin pages and API endpoints from indexing
Disallow: /admin
Disallow: /admin/*
Disallow: /api/
Disallow: /api/*

# Block uploads directory (temporary files)
Disallow: /uploads/
Disallow: /downloads/
Disallow: /compressed/
Disallow: /converted/

# Block private files
Disallow: /*.log$
Disallow: /*.json$
Disallow: /*.sqlite$

# Allow specific directories
Allow: /uploads/blog/

# Sitemap location
Sitemap: https://smallpdf.us/sitemap.xml

# Crawl delay (optional - prevents server overload)
Crawl-delay: 1

# Specific rules for Google
User-agent: Googlebot
Allow: /
Disallow: /admin
Disallow: /api/

# Specific rules for Bing
User-agent: Bingbot
Allow: /
Disallow: /admin
Disallow: /api/

# Block bad bots (optional)
User-agent: AhrefsBot
Crawl-delay: 10

User-agent: SemrushBot
Crawl-delay: 10

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /